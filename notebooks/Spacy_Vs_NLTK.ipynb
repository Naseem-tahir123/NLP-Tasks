{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Spacy Vs NLTK**\n",
        "### **Spacy**\n",
        "\n",
        "\n",
        "*   Spacy is object Oriented\n",
        "*   Spacy is user friendly\n",
        "\n",
        "*   New Library and have active user community\n",
        "*   Perfect For App Developers\n",
        "\n",
        "\n",
        "\n",
        "### **NLTK**\n",
        "\n",
        "\n",
        "*   NLTK is mainly a string processing library\n",
        "*   User friendly but probably less as compared to Spacy\n",
        "\n",
        "*   Old Library and user community is not active as spacy\n",
        "*   Perfect For Researchers\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YG5pVkYInB5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Spacy**"
      ],
      "metadata": {
        "id": "BZ50_h2ylttZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPOtqwmSjdJi"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Spliting Up Paragraph Into Sentence using spacy*"
      ],
      "metadata": {
        "id": "yOdE5Y1_khUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"My name is Naseem Tahir. I belong To Gilgit Baltistan\")\n",
        "for sentence in doc.sents:\n",
        "  print(sentence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Yx5QA4zkAvk",
        "outputId": "a23826c7-dc3d-48b4-b47c-2e954d92376a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My name is Naseem Tahir.\n",
            "I belong To Gilgit Baltistan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Spliting Up Sentence Into Words using spacy*"
      ],
      "metadata": {
        "id": "qEqgsaE_kvIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in doc.sents:\n",
        "  for word in sentence:\n",
        "    print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIr1RWUFk69y",
        "outputId": "61aaf79b-6b8f-4fb3-a686-5817c54440df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My\n",
            "name\n",
            "is\n",
            "Naseem\n",
            "Tahir\n",
            ".\n",
            "I\n",
            "belong\n",
            "To\n",
            "Gilgit\n",
            "Baltistan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **NLTK (Natural Language Tool Kit)**"
      ],
      "metadata": {
        "id": "VG8GqlBjlzlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt_tab\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzeWtvbcl67k",
        "outputId": "fac2e40b-245a-4a9e-af45-bf838459ca45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Spliting Up Paragraph Into Sentence using NLTK*"
      ],
      "metadata": {
        "id": "ZndznBZzmo9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sent_tokenize(\"My name is Naseem Tahir. I belong To Gilgit Baltistan\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSOpGeTBl8y8",
        "outputId": "975ddebb-cdc8-48d7-ab15-d60670d13a3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['My name is Naseem Tahir.', 'I belong To Gilgit Baltistan']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Spliting Up Sentence Into words using NLTK*"
      ],
      "metadata": {
        "id": "xt-Ai-kEmsgc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(\"My name is Naseem Tahir. I belong To Gilgit Baltistan\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2UwVbG4mWqv",
        "outputId": "bbea3a4c-6ef0-4125-bb0f-00f9d2e91428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['My',\n",
              " 'name',\n",
              " 'is',\n",
              " 'Naseem',\n",
              " 'Tahir',\n",
              " '.',\n",
              " 'I',\n",
              " 'belong',\n",
              " 'To',\n",
              " 'Gilgit',\n",
              " 'Baltistan']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tokenization In Spacy**"
      ],
      "metadata": {
        "id": "2F5GDLtnxu59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "ckxHDpWqx1Sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"en\")\n",
        "doc = nlp(\"Language is a thing of beauty. But mastering a new language from scratch is quite a daunting prospect.\")\n",
        "for token in doc:\n",
        "  print(token )"
      ],
      "metadata": {
        "id": "zTs6-iQfx3fS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e371289e-3eb4-4fe6-c22c-0452b1a2a098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Language\n",
            "is\n",
            "a\n",
            "thing\n",
            "of\n",
            "beauty\n",
            ".\n",
            "But\n",
            "mastering\n",
            "a\n",
            "new\n",
            "language\n",
            "from\n",
            "scratch\n",
            "is\n",
            "quite\n",
            "a\n",
            "daunting\n",
            "prospect\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp('''\"Let's go to N.Y.!\"''')\n",
        "for token in doc:\n",
        "  print(token )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eowQr7OSzwlA",
        "outputId": "bcda33df-4e55-4ff8-b911-05ec2716f66e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"\n",
            "Let\n",
            "'s\n",
            "go\n",
            "to\n",
            "N.Y.\n",
            "!\n",
            "\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(doc)"
      ],
      "metadata": {
        "id": "0FsG-nIjy5IK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d885e50c-2883-4d97-d01b-beeda497605d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.doc.Doc"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okjmX5Nt6Ems",
        "outputId": "10ff53dc-5e67-4e5e-92b5-f51498db8c64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.token.Token"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "span = doc[1:5]\n",
        "type(span)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfl0TmWD61ao",
        "outputId": "0b120d91-dd15-415d-bc4b-b5911d74d8f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.span.Span"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir(token)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enjFC9mC6_A0",
        "outputId": "5e96a25c-f942-4e7b-cb29-46d58eba2213"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['_',\n",
              " '__bytes__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__getstate__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__len__',\n",
              " '__lt__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__pyx_vtable__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__unicode__',\n",
              " 'ancestors',\n",
              " 'check_flag',\n",
              " 'children',\n",
              " 'cluster',\n",
              " 'conjuncts',\n",
              " 'dep',\n",
              " 'dep_',\n",
              " 'doc',\n",
              " 'ent_id',\n",
              " 'ent_id_',\n",
              " 'ent_iob',\n",
              " 'ent_iob_',\n",
              " 'ent_kb_id',\n",
              " 'ent_kb_id_',\n",
              " 'ent_type',\n",
              " 'ent_type_',\n",
              " 'get_extension',\n",
              " 'has_dep',\n",
              " 'has_extension',\n",
              " 'has_head',\n",
              " 'has_morph',\n",
              " 'has_vector',\n",
              " 'head',\n",
              " 'i',\n",
              " 'idx',\n",
              " 'iob_strings',\n",
              " 'is_alpha',\n",
              " 'is_ancestor',\n",
              " 'is_ascii',\n",
              " 'is_bracket',\n",
              " 'is_currency',\n",
              " 'is_digit',\n",
              " 'is_left_punct',\n",
              " 'is_lower',\n",
              " 'is_oov',\n",
              " 'is_punct',\n",
              " 'is_quote',\n",
              " 'is_right_punct',\n",
              " 'is_sent_end',\n",
              " 'is_sent_start',\n",
              " 'is_space',\n",
              " 'is_stop',\n",
              " 'is_title',\n",
              " 'is_upper',\n",
              " 'lang',\n",
              " 'lang_',\n",
              " 'left_edge',\n",
              " 'lefts',\n",
              " 'lemma',\n",
              " 'lemma_',\n",
              " 'lex',\n",
              " 'lex_id',\n",
              " 'like_email',\n",
              " 'like_num',\n",
              " 'like_url',\n",
              " 'lower',\n",
              " 'lower_',\n",
              " 'morph',\n",
              " 'n_lefts',\n",
              " 'n_rights',\n",
              " 'nbor',\n",
              " 'norm',\n",
              " 'norm_',\n",
              " 'orth',\n",
              " 'orth_',\n",
              " 'pos',\n",
              " 'pos_',\n",
              " 'prefix',\n",
              " 'prefix_',\n",
              " 'prob',\n",
              " 'rank',\n",
              " 'remove_extension',\n",
              " 'right_edge',\n",
              " 'rights',\n",
              " 'sent',\n",
              " 'sent_start',\n",
              " 'sentiment',\n",
              " 'set_extension',\n",
              " 'set_morph',\n",
              " 'shape',\n",
              " 'shape_',\n",
              " 'similarity',\n",
              " 'subtree',\n",
              " 'suffix',\n",
              " 'suffix_',\n",
              " 'tag',\n",
              " 'tag_',\n",
              " 'tensor',\n",
              " 'text',\n",
              " 'text_with_ws',\n",
              " 'vector',\n",
              " 'vector_norm',\n",
              " 'vocab',\n",
              " 'whitespace_']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tk1 = doc[3]\n",
        "tk1.is_alpha"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khjstfbo7boy",
        "outputId": "31996a03-007a-4d25-eddf-7de3fe0478c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(token, \"-->\" , 'index ', token.i,\n",
        "        'is_alpha:', token.is_alpha,\n",
        "        'is_punct: ', token.is_punct,\n",
        "        'is_number', token.like_num,\n",
        "        'is_Currency', token.is_currency)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCzSyevU9-Jm",
        "outputId": "f30d0bc0-3e2f-455d-af24-4d5d1df4525d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\" --> index  0 is_alpha: False is_punct:  True is_number False\n",
            "Let --> index  1 is_alpha: True is_punct:  False is_number False\n",
            "'s --> index  2 is_alpha: False is_punct:  False is_number False\n",
            "go --> index  3 is_alpha: True is_punct:  False is_number False\n",
            "to --> index  4 is_alpha: True is_punct:  False is_number False\n",
            "N.Y. --> index  5 is_alpha: False is_punct:  False is_number False\n",
            "! --> index  6 is_alpha: False is_punct:  True is_number False\n",
            "\" --> index  7 is_alpha: False is_punct:  True is_number False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "email_txt = nlp('''For customer support, you can reach us at support@companydomain.com\n",
        " or helpdesk@servicehub.com. For sales inquiries, email sales@marketplus.com,\n",
        "  info@saleshub.io, or business@ecommerce.net. If you're submitting a resume,\n",
        "   send it to careers@techfirm.co,hr@globaljobs.org, or recruitment@talenthub.biz.''')"
      ],
      "metadata": {
        "id": "lg-m2z0YL1Ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Email Extraction by using spacy**"
      ],
      "metadata": {
        "id": "ubTqZ4nVMWdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in email_txt:\n",
        "  if token.like_email:\n",
        "    print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BkrlmRnL_mu",
        "outputId": "6a464245-d608-4b38-faff-5064ab59a221"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "support@companydomain.com\n",
            "helpdesk@servicehub.com\n",
            "sales@marketplus.com\n",
            "info@saleshub.io\n",
            "business@ecommerce.net\n",
            "careers@techfirm.co,hr@globaljobs.org\n",
            "recruitment@talenthub.biz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Urdu Sentence Tokenization Using Spacy**"
      ],
      "metadata": {
        "id": "KcuqtRGXQabn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank('ur')\n",
        "urdu_text = nlp('''سردیوں میں جب ہم گیزر استعمال کرتے ہیں تو سب سے زیادہ یہ مسئلہ ہوتا ہے\n",
        "کہ گیزر اسٹارٹ ہی نہیں ہو رہا ہوتا- ''')\n",
        "token = [token.text for token in urdu_text]\n",
        "token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7x-dQRfOBUz",
        "outputId": "d2e8a591-28a9-479b-d025-ef82108aaa26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['سردیوں',\n",
              " 'میں',\n",
              " 'جب',\n",
              " 'ہم',\n",
              " 'گیزر',\n",
              " 'استعمال',\n",
              " 'کرتے',\n",
              " 'ہیں',\n",
              " 'تو',\n",
              " 'سب',\n",
              " 'سے',\n",
              " 'زیادہ',\n",
              " 'یہ',\n",
              " 'مسئلہ',\n",
              " 'ہوتا',\n",
              " 'ہے',\n",
              " '\\n',\n",
              " 'کہ',\n",
              " 'گیزر',\n",
              " 'اسٹارٹ',\n",
              " 'ہی',\n",
              " 'نہیں',\n",
              " 'ہو',\n",
              " 'رہا',\n",
              " 'ہوتا-']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Language Processing Pipeline In Spacy**"
      ],
      "metadata": {
        "id": "UeZ8N7DnUIYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.pipe_names\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsO57RceUOyV",
        "outputId": "2da3b10d-5f06-4fd1-cf08-85ed9ac00b68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt = nlp(\"Apple is looking at buying U.K. startup for $1 billion.\")\n",
        "for token in txt:\n",
        "  print(token, \"|\", token.pos_, \"|\", token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOy13xRtUi95",
        "outputId": "d0b84bba-a7e6-4728-d9bc-717a46e7234e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple | PROPN | Apple\n",
            "is | AUX | be\n",
            "looking | VERB | look\n",
            "at | ADP | at\n",
            "buying | VERB | buy\n",
            "U.K. | PROPN | U.K.\n",
            "startup | NOUN | startup\n",
            "for | ADP | for\n",
            "$ | SYM | $\n",
            "1 | NUM | 1\n",
            "billion | NUM | billion\n",
            ". | PUNCT | .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt2 = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")\n",
        "for ent in txt2.ents:\n",
        "  print(ent.text, \"|\", ent.label_, \"|\", spacy.explain(ent.label_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWtXsI2ZV9nq",
        "outputId": "76a1a0fd-da26-41dc-c833-4f5bbe9cd39c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla Inc | ORG | Companies, agencies, institutions, etc.\n",
            "$45 billion | MONEY | Monetary values, including unit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "displacy.render(txt2, style='ent ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Ui0uxJi4W8eV",
        "outputId": "a771177b-71e4-4ed8-da84-86070f6d78ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Tesla Inc\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " is going to acquire twitter for \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    $45 billion\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
              "</mark>\n",
              "</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Customize Blank pipeline**"
      ],
      "metadata": {
        "id": "YtdBYmZoYIeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "source_nlp = spacy.load('en_core_web_sm')\n",
        "nlp = spacy.blank('en')\n",
        "nlp.add_pipe('ner', source = source_nlp)\n",
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94M1pX0NYNU6",
        "outputId": "4d6774cb-a6b3-4b33-f3e3-aa565be79828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ner']"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt3 = nlp('Pakistan came into being on 14 August 1946')\n",
        "for ent in txt3.ents:\n",
        "  print(ent.text, \"|\", ent.label_, \"|\", spacy.explain(ent.label_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gec8ms7VYjzb",
        "outputId": "7cb2d418-fbab-4d5f-a0ad-0aff550221b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pakistan | GPE | Countries, cities, states\n",
            "14 August 1946 | DATE | Absolute or relative dates or periods\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Stemming And Lemmatization**"
      ],
      "metadata": {
        "id": "iENS1XtCeKZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Stemming**\n",
        "### Spacy does'nt have support for stemming"
      ],
      "metadata": {
        "id": "X3ef0FLvfApq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "StP3DRSWeRp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "words  = ['eating', 'eats', 'eat', 'ate', 'adjustable', 'rafting', 'ability', 'meeting']\n",
        "stem_words = [stemmer.stem(word) for word in words]\n",
        "stem_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WE0PUkYFeVAB",
        "outputId": "d0867a1b-d852-4910-c26b-b38a6fc5b740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['eat', 'eat', 'eat', 'ate', 'adjust', 'raft', 'abil', 'meet']"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lemmatization**"
      ],
      "metadata": {
        "id": "ocf6iDahfViJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "txt = nlp('eating eats eat ate adjustable rafting ability meeting better')\n",
        "for token in txt:\n",
        "  print(token, \"|\", token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXuuqbVhfszP",
        "outputId": "95a9599d-514c-428e-a835-2a4d93fb2bdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating | eat\n",
            "eats | eat\n",
            "eat | eat\n",
            "ate | eat\n",
            "adjustable | adjustable\n",
            "rafting | raft\n",
            "ability | ability\n",
            "meeting | meeting\n",
            "better | well\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ribXifVmhBrR",
        "outputId": "c88671e9-4984-46e2-f69a-dde0d6b1fc76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt1 = nlp('Bro, you wanna go? Brah, don\\'t say no! I am exhausted')\n",
        "ar = nlp.get_pipe('attribute_ruler')\n",
        "ar.add([[{\"TEXT\":\"Bro\"}],[{\"TEXT\":\"Brah\"}]],{\"LEMMA\":\"Brother\"})\n",
        "for token in txt1:\n",
        "  print(token, \"|\", token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMRONnilhE0z",
        "outputId": "a7d6e7ae-03f0-45d2-c897-a509a3060b9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bro | Brother\n",
            ", | ,\n",
            "you | you\n",
            "wanna | wanna\n",
            "go | go\n",
            "? | ?\n",
            "Brah | Brother\n",
            ", | ,\n",
            "do | do\n",
            "n't | not\n",
            "say | say\n",
            "no | no\n",
            "! | !\n",
            "I | I\n",
            "am | be\n",
            "exhausted | exhaust\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Parts Of Speech (POS) Tagging**"
      ],
      "metadata": {
        "id": "J8FtZzTttslE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "txt = nlp(\"My name is Naseem Tahir, I have completed Bachelors in the field of (computer science) from Federal Urdu University\")\n",
        "for word in txt:\n",
        "  print (word, \"|\", word.pos_, \"|\", spacy.explain(word.pos_) , word.tag_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWlpOqLAt0Qb",
        "outputId": "d14f53f9-1f3d-4bd3-efcc-2c2e589281c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My | PRON | pronoun\n",
            "name | NOUN | noun\n",
            "is | AUX | auxiliary\n",
            "Naseem | PROPN | proper noun\n",
            "Tahir | PROPN | proper noun\n",
            ", | PUNCT | punctuation\n",
            "I | PRON | pronoun\n",
            "have | AUX | auxiliary\n",
            "completed | VERB | verb\n",
            "Bachelors | PROPN | proper noun\n",
            "in | ADP | adposition\n",
            "the | DET | determiner\n",
            "field | NOUN | noun\n",
            "of | ADP | adposition\n",
            "( | PUNCT | punctuation\n",
            "computer | NOUN | noun\n",
            "science | NOUN | noun\n",
            ") | PUNCT | punctuation\n",
            "from | ADP | adposition\n",
            "Federal | PROPN | proper noun\n",
            "Urdu | PROPN | proper noun\n",
            "University | PROPN | proper noun\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Removing unnecessary parameters from text like punctuations, space etc**"
      ],
      "metadata": {
        "id": "vZj9nk4gyHMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_words = []\n",
        "for word in txt:\n",
        "  if word.pos_ not in ['PUNCT', 'SPACE']:\n",
        "    clean_words.append(word)\n",
        "clean_words\n",
        "    # print (word, \"|\", word.pos_, \"|\", spacy.explain(word.pos_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Srm05-7ou8nQ",
        "outputId": "f2116749-68db-4529-9160-f901ff813661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[My,\n",
              " name,\n",
              " is,\n",
              " Naseem,\n",
              " Tahir,\n",
              " I,\n",
              " have,\n",
              " completed,\n",
              " Bachelors,\n",
              " in,\n",
              " the,\n",
              " field,\n",
              " of,\n",
              " computer,\n",
              " science,\n",
              " from,\n",
              " Federal,\n",
              " Urdu,\n",
              " University]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Count the number of occurance of Noun, Pronoun, Verb ... etc in a sentence of para**"
      ],
      "metadata": {
        "id": "5IgsMshJxzM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count = txt.count_by(spacy.attrs.POS)\n",
        "count\n",
        "for k,v in count.items():\n",
        "  print(txt.vocab[k].text, \"|\", v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgPEZULTwES2",
        "outputId": "19ac5420-ca14-474e-dc3b-29d67bd54e9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PRON | 2\n",
            "NOUN | 4\n",
            "AUX | 2\n",
            "PROPN | 6\n",
            "PUNCT | 3\n",
            "VERB | 1\n",
            "ADP | 3\n",
            "DET | 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Named Entity Recognition (NER)**"
      ],
      "metadata": {
        "id": "MIHEiexwAJe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "txt = nlp(\"My name is Naseem Tahir, I have completed Bachelors in the field of (computer science) from Federal Urdu University\")\n",
        "for ent in txt.ents:\n",
        "  print(ent.text, \"|\", ent.label_, \"|\", spacy.explain(ent.label_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTXObjgRARYn",
        "outputId": "6bc2f80a-bd3b-4f34-be8c-a9b4c13c7646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naseem Tahir | PERSON | People, including fictional\n",
            "Federal Urdu University | ORG | Companies, agencies, institutions, etc.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Tesla Inc is going to acquire twitter for $45 billion\")"
      ],
      "metadata": {
        "id": "ZTOkw-5wAc_3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b616324f-1f42-4e45-a1e0-2dfdbc93a46b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tesla"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in doc.ents:\n",
        "  print(ent.text, \"|\" , ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b9W_vX7b3Nw",
        "outputId": "68923eb1-e94c-444f-bd41-8cc53cc59b62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla Inc | ORG\n",
            "$45 billion | MONEY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In the above output, \"Twitter\" is not recognized as an organization (ORG).\n",
        "# We can manually modify the entities to correctly label \"Twitter\" as an organization.\n",
        "from spacy.tokens import Span\n",
        "s1 = Span(doc, 0, 1, label = \"ORG\")\n",
        "s2 = Span(doc, 6, 7, label = 'ORG')\n",
        "doc.set_ents([s1, s2], default = 'unmodified')\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, \"|\", ent.label_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQo-BnmacbcM",
        "outputId": "0ad190fd-df1d-4053-f814-8754766256ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla Inc | ORG\n",
            "twitter | ORG\n",
            "$45 billion | MONEY\n"
          ]
        }
      ]
    }
  ]
}